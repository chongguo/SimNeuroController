{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrating to tensor regression\n",
    "\n",
    "The EVs are unexpectedly bad across different neural networks. This could either be real difference in the expressed function spaces but it's also likely that we are mis-specifying the source-target model. One obvious issue is that vectorization below fully connected layers destroys a lot of interal structure in the tensor output of convolution layers. Moreoever, vectorization of tensors of even moderate dimensions exponentially inflate the number of predictive variables. For these reasons we should consider low-rank or regularized tensor regression. Here is an evaluation of tensorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorly.base import tensor_to_vec, partial_tensor_to_vec\n",
    "from tensorly.random import check_random_state\n",
    "from tensorly.regression.cp_regression import CPRegressor\n",
    "from tensorly.regression.tucker_regression import TuckerRegressor\n",
    "import tensorly as tl\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torchvision.models as models\n",
    "from util.modelregressions import CNNCrossFit\n",
    "from util.misc_functions import float2rgb\n",
    "from dataset.hvm import HVMDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import os\n",
    "\n",
    "tl.set_backend('pytorch')\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and preprocessing hvm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:39<00:00, 126.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:18<00:00, 26.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# load up target and source models\n",
    "squeezenet_target = models.squeezenet1_1(pretrained=True).features\n",
    "squeezenet_source = models.squeezenet1_0(pretrained=True).features\n",
    "# define basic params \n",
    "target_layer = 12\n",
    "source_layer = 12\n",
    "source_channels = np.arange(512)\n",
    "target_channels = np.arange(100)\n",
    "# pre-load data into memory for speed\n",
    "n_data = 5000;\n",
    "n_train = 4000;\n",
    "n_test = 1000;\n",
    "hvmdataset = HVMDataset('cpu',n_data)\n",
    "# create a control experiment (squeezenet1_1-->squeezenet1_0)\n",
    "snet2snet = CNNCrossFit(squeezenet_target,squeezenet_source,target_layer,source_layer,target_channels,source_channels,device)\n",
    "# extract layer activation across two models\n",
    "snet2snet.design(hvmdataset, batch_size = 10, shuffle = False)\n",
    "# train test split\n",
    "snet2snet.train_test_split(n_train,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract X and Y tensor\n",
    "X_train = tl.tensor(snet2snet.sor_train,device=device)\n",
    "X_test = tl.tensor(snet2snet.sor_test,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5d2dc0ab81ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrand_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnet2snet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnet2snet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvalididx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnet2snet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnet2snet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnet2snet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnet2snet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mrand_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrand_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalididx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_units\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "n_units = 200\n",
    "rand_set = random.sample(range(0,len(snet2snet.indexer)),len(snet2snet.indexer))\n",
    "valididx = np.where((snet2snet.indexer.get_tar(rand_set)\n",
    "rand_units = [rand_set[u] for i,u in enumerate(valididx[:n_units])]\n",
    "n_units = len(rand_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare rank-1 Tensor Regression to Orthogonal Matching Pursuit because that had the second best EV score in Jonas's original experiments on Ko's data and doesn't need much tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score as EV\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, OrthogonalMatchingPursuit\n",
    "\n",
    "ncs = [1,2,3,4,5,6,7,8]\n",
    "n_ncs = len(ncs)\n",
    "\n",
    "rw = np.zeros((n_ncs, n_units,snet2snet.n_sor,15,15))\n",
    "X_train_LR = snet2snet.sor_train.reshape(n_train,-1)\n",
    "X_test_LR = snet2snet.sor_test.reshape(n_test,-1)\n",
    "Y_pred_test_LR = np.zeros((n_test,n_units,n_ncs))\n",
    "Y_pred_train_LR = np.zeros((n_train,n_units,n_ncs))\n",
    "\n",
    "\n",
    "test_score_LR = np.zeros((n_units,n_ncs))\n",
    "train_score_LR = np.zeros((n_units,n_ncs))\n",
    "for i,u in enumerate(tqdm(rand_units)):\n",
    "    index = snet2snet.indexer.get_sur(u)\n",
    "    Y_train_LR = snet2snet.tar_train[:,index[0],index[1],index[2]]\n",
    "    Y_test_LR = snet2snet.tar_test[:,index[0],index[1],index[2]]\n",
    "    for j, nc in enumerate(ncs):\n",
    "        estimator_LR = OrthogonalMatchingPursuit(n_nonzero_coefs=nc)\n",
    "        #estimator_LR = BayesianRidge()\n",
    "        estimator_LR.fit(X_train_LR, Y_train_LR)\n",
    "        rw[j,i] = estimator_LR.coef_.reshape(snet2snet.n_sor,15,15)\n",
    "        # score on train and test set\n",
    "        Y_pred_test_LR[:,i,j] = estimator_LR.predict(X_test_LR)\n",
    "        Y_pred_train_LR[:,i,j] = estimator_LR.predict(X_train_LR)\n",
    "        test_score_LR[i,j] = EV(Y_test_LR,Y_pred_test_LR[:,i,j])\n",
    "        train_score_LR[i,j] = EV(Y_train_LR,Y_pred_train_LR[:,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.median(train_score_LR,axis=0))\n",
    "plt.plot(np.median(test_score_LR,axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = np.zeros((n_units,snet2snet.n_sor,15,15))\n",
    "ttw = []\n",
    "test_score = np.zeros(n_units)\n",
    "train_score = np.zeros(n_units)\n",
    "Y_pred_test = tl.tensor(np.zeros((n_test,n_units)),device=device)\n",
    "Y_pred_train = tl.tensor(np.zeros((n_train,n_units)),device=device)\n",
    "Y_test = tl.tensor(np.zeros((n_test,n_units)),device=device)\n",
    "Y_train = tl.tensor(np.zeros((n_train,n_units)),device=device)\n",
    "for i,u in enumerate(tqdm(rand_units)):\n",
    "    index = snet2snet.indexer.get_sur(u)\n",
    "    Y_train[:,i] = tl.tensor(snet2snet.tar_train[:,index[0],index[1],index[2]],device=device)\n",
    "    Y_test[:,i] = tl.tensor(snet2snet.tar_test[:,index[0],index[1],index[2]],device=device)\n",
    "    # Create a Tucker Regressor estimator\n",
    "    estimator = TuckerRegressor(weight_ranks=[1,1,1], tol=1e-5, n_iter_max=50, reg_W=10, verbose=0)\n",
    "    # Fit the estimator to the dat\n",
    "    estimator.fit(X_train, Y_train[:,i])\n",
    "    # score on train and test set\n",
    "    Y_pred_test[:,i] = estimator.predict(X_test)\n",
    "    Y_pred_train[:,i] = estimator.predict(X_train)\n",
    "    tw[i,:,:,:] = estimator.weight_tensor_.cpu().detach().squeeze().numpy()\n",
    "    ttw.append(estimator.tucker_weight_)\n",
    "    test_score[i] = EV(Y_test[:,i].cpu().detach().numpy(),Y_pred_test[:,i].cpu().detach().numpy())\n",
    "    train_score[i] = EV(Y_train[:,i].cpu().detach().numpy(),Y_pred_train[:,i].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 4\n",
    "\n",
    "i = np.where(test_score>.25)[0][0]\n",
    "idx = snet2snet.indexer.get_sur(rand_units[i])\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "ax0 = plt.subplot(2,3,1)\n",
    "plt.plot(Y_train_LR,Y_pred_train_LR[:,i,comp],'.')\n",
    "plt.plot(Y_test_LR,Y_pred_test_LR[:,i,comp],'.')\n",
    "plt.plot([-100,100],[-100,100],'k--')\n",
    "plt.xlim([0,10])\n",
    "plt.ylim([0,10])\n",
    "ax0.set_aspect('equal')\n",
    "plt.xlabel('target activation')\n",
    "plt.ylabel('OMP predicted activation')\n",
    "\n",
    "ax1 = plt.subplot(2,3,4)\n",
    "plt.plot(Y_train[:,i].cpu(),Y_pred_train[:,i].cpu(),'.')\n",
    "plt.plot(Y_test[:,i].cpu(),Y_pred_test[:,i].cpu(),'.')\n",
    "plt.plot([-100,100],[-100,100],'k--')\n",
    "plt.xlim([0,10])\n",
    "plt.ylim([0,10])\n",
    "ax1.set_aspect('equal')\n",
    "plt.xlabel('target activation')\n",
    "plt.ylabel('Tensor predicted activation')\n",
    "\n",
    "ax2 = plt.subplot(2,3,2)\n",
    "plt.imshow(np.sum(rw[comp,i],axis=0).T)\n",
    "plt.plot(idx[1],idx[2],'rx')\n",
    "ax2.get_xaxis().set_ticks([])\n",
    "ax2.axes.get_yaxis().set_ticks([])\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('h')\n",
    "\n",
    "ax3 = plt.subplot(2,3,5)\n",
    "plt.imshow((ttw[i][1][1].cpu().T*ttw[i][1][2].cpu())**2)\n",
    "plt.plot(idx[1],idx[2],'rx')\n",
    "ax3.get_xaxis().set_ticks([])\n",
    "ax3.axes.get_yaxis().set_ticks([])\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('h')\n",
    "\n",
    "ax4 = plt.subplot(2,3,3)\n",
    "plt.plot(np.sum(np.sum(rw[comp,i],axis=1),axis=1))\n",
    "ax4.get_xaxis().set_ticks([])\n",
    "ax4.axes.get_yaxis().set_ticks([])\n",
    "plt.xlim([0,snet2snet.n_sor])\n",
    "plt.ylabel('weight')\n",
    "\n",
    "ax5 = plt.subplot(2,3,6)\n",
    "plt.plot(ttw[i][1][0].cpu())\n",
    "ax5.get_xaxis().set_ticks([])\n",
    "ax5.axes.get_yaxis().set_ticks([])\n",
    "plt.xlim([0,snet2snet.n_sor])\n",
    "plt.ylabel('weight')\n",
    "plt.xlabel('feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_aspect('equal')\n",
    "plt.scatter(train_score_LR, test_score_LR, s=10, label='OMP')\n",
    "plt.scatter(train_score, test_score, s=10, label='Tensor')\n",
    "plt.plot([-1,1],[-1,1],'k--')\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel(r'${f_{source}(x),f_{target}(x)}$  train EV', fontsize=10)\n",
    "plt.ylabel(r'${f_{source}(x),f_{target}(x)}$  test EV', fontsize=10)\n",
    "plt.legend()\n",
    "ax1 = plt.subplot(1,2,2)\n",
    "ax1.set_aspect('equal')\n",
    "plt.plot([-1,1],[-1,1],'k--')\n",
    "plt.scatter(test_score_LR, test_score,s=10)\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel(r'OMP test EV', fontsize=10)\n",
    "plt.ylabel(r'Tensor  test EV', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = snet2snet.indexer.get_sur(rand_units[499])\n",
    "plt.hist(snet2snet.Xtar[:,idx[0],idx[1],idx[2]],100)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_score,bins=np.arange(-1,1,.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_LR[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
